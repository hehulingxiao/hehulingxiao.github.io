---
title: "SPIRIT: Style-guided Patch Interaction for Fashion Image Retrieval with Text Feedback"  
collection: publications  
date: 2024-02-07   
venue: 'TOMM'  
paperurl: 'http://39.108.48.32/mipl/download_paper.php?fileId=202402'
citation: 'Yanzhe Chen, Jiahuan Zhou and Yuxin Peng, "SPIRIT: Style-guided Patch Interaction for Fashion Image Retrieval with Text Feedback", TOMM 2024.'
excerpt: 'TL;DR: This paper proposes a modeling method for interaction between local patches of fashion images, which can be well applied to fine-grained composed fashion image retrieval.'
---


**Abstract:** Fashion image retrieval with text feedback aims to find the target image according to the reference image and the modification from the user. This is a challenging task as it requires not only the synergistic understanding of both visual and textual modalities, but also the ability to model a wide variety of styles that fashion images contain. Hence, the crucial aspect of addressing this problem lies in exploiting the abundant semantic information inherent in fashion images and correlating it with the textual description of style. 
Recognizing that style is generally situated at the local level, we explicitly define style as the commonalities and differences between local areas of fashion images. Building upon this, we propose a Style-guided Patch InteRaction approach for fashion Image retrieval with Text feedback (SPIRIT), which focuses on the decisive influence of local details of fashion images on their style. Three corresponding networks are designed pertinently.
The Patch-level Style Commonality (PSC) network is introduced to fully leverage the semantic information among patches and compute their average as the style commonality. Subsequently, the Patch-level Style Difference (PSD) network employs a graph reasoning network to model the patch-level difference and filter out insignificant patches. By considering the above two networks, mutual information about style is obtained from the interaction between patches. Finally, the Visual Textual Fusion (VTF) network is utilized to integrate visual features with rich semantic information and textual features. Experimental results on four benchmark datasets demonstrate that our proposed SPIRIT achieves state-of-the-art performance.  
